{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : Importing all the libraries & files, and defining all the neccesary functions to perform the Text Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requried imports\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "import numpy as np\n",
    "from goose3 import Goose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File locations of all the required files needed\n",
    "\n",
    "inputData = 'D:\\Data Science\\Text Analysis\\Input.xlsx' \n",
    "\n",
    "positiveWords = 'D:/Data Science/Text Analysis\\MasterDictionary/positive-words.txt'\n",
    "negativeWords = 'D:/Data Science/Text Analysis\\MasterDictionary/negative-words.txt'\n",
    "\n",
    "stopWordsAuditor = 'D:/Data Science/Text Analysis/StopWords/StopWords_Auditor.txt'\n",
    "stopWordsCurrencies = 'D:/Data Science/Text Analysis/StopWords/StopWords_Currencies.txt'\n",
    "stopWordsDatesAndNum = 'D:/Data Science/Text Analysis/StopWords/StopWords_DatesandNumbers.txt'\n",
    "stopWordsGenric = 'D:/Data Science/Text Analysis/StopWords/StopWords_Generic.txt'\n",
    "stopWordsGenericLong = 'D:/Data Science/Text Analysis/StopWords/StopWords_GenericLong.txt'\n",
    "stopWordsGeographic = 'D:/Data Science/Text Analysis/StopWords/StopWords_Geographic.txt'\n",
    "stopWordsNames = 'D:/Data Science/Text Analysis/StopWords/StopWords_Names.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch Articles from the given links .\n",
    "\n",
    "def getArticles(dataframe, URL):\n",
    "    p = [] \n",
    "    for i in range(df.shape[0]):\n",
    "        url = dataframe.URL[i]\n",
    "        article = Goose().extract(url=url)\n",
    "        article = str(article.cleaned_text)\n",
    "        p.append(article)\n",
    "\n",
    "    dataframe['ARTICLE'] = p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the files....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stopWord Auditor to a list.\n",
    "\n",
    "with open(stopWordsAuditor ,'r') as stop_words_auditor:\n",
    "    stop_words_auditor = stop_words_auditor.read().lower()\n",
    "stop_words_auditor = stop_words_auditor.split('\\n')\n",
    "stop_words_auditor[-1:] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stopWord Currencies to a list.\n",
    "\n",
    "with open(stopWordsCurrencies ,'r') as stop_words_curren:\n",
    "    stop_words_curren = stop_words_curren.read().lower()\n",
    "stop_words_curren = stop_words_curren.split('\\n')\n",
    "stop_words_curren[-1:] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the stopWord Dates and Number to a list.\n",
    "\n",
    "with open(stopWordsDatesAndNum ,'r') as stop_words_DateAndNum:\n",
    "    stop_words_DateAndNum = stop_words_DateAndNum.read().lower()\n",
    "stop_words_DateAndNum = stop_words_DateAndNum.split('\\n')\n",
    "stop_words_DateAndNum[-1:] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stopWord Genric to a list.\n",
    "\n",
    "with open(stopWordsGenric ,'r') as stop_words_generic:\n",
    "    stop_words_generic = stop_words_generic.read().lower()\n",
    "stop_words_generic = stop_words_generic.split('\\n')\n",
    "stop_words_generic[-1:] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stopWord Generic Long to a list.\n",
    "\n",
    "with open(stopWordsGenericLong ,'r') as stop_words_genericLong:\n",
    "    stop_words_genericLong = stop_words_genericLong.read().lower()\n",
    "stop_words_genericLong = stop_words_genericLong.split('\\n')\n",
    "stop_words_genericLong[-1:] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stopWord Geographic to a list.\n",
    "\n",
    "with open(stopWordsGeographic ,'r') as stop_words_geographic:\n",
    "    stop_words_geographic = stop_words_geographic.read().lower()\n",
    "stop_words_geographic = stop_words_geographic.split('\\n')\n",
    "stop_words_geographic[-1:] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stopWord Name to a list.\n",
    "\n",
    "with open(stopWordsNames ,'r') as stop_words_names:\n",
    "    stop_words_names = stop_words_names.read().lower()\n",
    "stop_words_names = stop_words_names.split('\\n')\n",
    "stop_words_names[-1:] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining alll the stopwords list to a single list.\n",
    "\n",
    "final_list = stop_words_names+stop_words_geographic+stop_words_auditor+stop_words_curren+stop_words_DateAndNum+stop_words_generic+stop_words_genericLong "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer : to filter out all the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "def tokenizer(text):\n",
    "    text = text.lower()\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    filtered_words = list(filter(lambda token: token not in final_list, tokens))\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Positive Words & Negative Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading positive words to a list\n",
    "\n",
    "with open(positiveWords,'r') as posfile:\n",
    "    positiveWords=posfile.read().lower()\n",
    "positiveWordList=positiveWords.split('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading negative words to a list\n",
    "\n",
    "with open(negativeWords ,'r') as negfile:\n",
    "    negativeWords=negfile.read().lower()\n",
    "negativeWordList=negativeWords.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating positive score \n",
    "\n",
    "def positive_score(text):\n",
    "    numPosWords = 0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in positiveWordList:\n",
    "            numPosWords  += 1\n",
    "    \n",
    "    sumPos = numPosWords\n",
    "    return sumPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Negative score\n",
    "\n",
    "def negative_score(text):\n",
    "    numNegWords=0\n",
    "    rawToken = tokenizer(text)\n",
    "    for word in rawToken:\n",
    "        if word in negativeWordList:\n",
    "            numNegWords -=1\n",
    "    sumNeg = numNegWords \n",
    "    sumNeg = sumNeg * -1\n",
    "    return sumNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Calculating polarity score\n",
    "def polarity_score(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "# Calculating Subjectivity score\n",
    "def subjectivity_score(text):\n",
    "    return TextBlob(text).sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Syllablle per Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     C:\\Users\\karti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\karti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import cmudict\n",
    "import nltk\n",
    "nltk.download('cmudict')\n",
    "nltk.download('punkt')\n",
    "d = cmudict.dict()\n",
    "\n",
    "# Calculating syllablle in a input text.\n",
    "\n",
    "def syllables(text):\n",
    "    count = 0\n",
    "    vowels = 'aeiouy'\n",
    "    text = text.lower()\n",
    "    if text[0] in vowels:\n",
    "        count +=1\n",
    "    for index in range(1,len(text)):\n",
    "        if text[index] in vowels and text[index-1] not in vowels:\n",
    "            count +=1\n",
    "    if text.endswith('e'):\n",
    "        count -= 1\n",
    "    if text.endswith('le'):\n",
    "        count+=1\n",
    "    if count == 0:\n",
    "        count +=1\n",
    "    return count\n",
    "\n",
    "# number-of-syllables-in-a-word\n",
    "\n",
    "def nsyl(word):\n",
    "    try:\n",
    "        return max([len(list(y for y in x if y[-1].isdigit())) for x in d[word.lower()]])\n",
    "    except KeyError:\n",
    "        #if word not found in cmudict\n",
    "        return syllables(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Average sentence length \n",
    "# It will calculated using formula --- Average Sentence Length = the number of words / the number of sentences\n",
    "     \n",
    "def average_sentence_length(text):\n",
    "    sentence_list = sent_tokenize(text)\n",
    "    tokens = tokenizer(text)\n",
    "    totalWordCount = len(tokens)\n",
    "    totalSentences = len(sentence_list)\n",
    "    average_sent = 0\n",
    "    if totalSentences != 0:\n",
    "        average_sent = totalWordCount / totalSentences\n",
    "    \n",
    "    average_sent_length = average_sent\n",
    "    \n",
    "    return round(average_sent_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Average Number of Words per Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcutating Average Number of Words per Sentence..\n",
    "# It is calculated using formula = sum_of_words in sentence / num_of _entences\n",
    "\n",
    "def avg_num_words_per_sentence(text):\n",
    "    parts = [len(l.split()) for l in re.split(r'[?!.]', text) if l.strip()]\n",
    "    return (sum(parts)/len(parts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcutating Average Word length.\n",
    "# It is calculated using the formula = total length of all words / total num of words..\n",
    "\n",
    "def average_word_length(text):\n",
    "    filtered = ''.join(filter(lambda x: x not in '\".,;!-()', text))\n",
    "    words = [word for word in filtered.split() if word]\n",
    "    avg = sum(map(len, words))/len(words)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Word Count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting complex words\n",
    "def complex_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complex_word_count = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        if nsyl(word.lower()) > 2:\n",
    "                    complex_word_count += 1\n",
    "    return complex_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage of Complex Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating percentage of complex word \n",
    "# It is calculated using Percentage of Complex words = the number of complex words / the number of words \n",
    "\n",
    "def percentage_complex_word(text):\n",
    "    tokens = tokenizer(text)\n",
    "    complex_word_count = 0\n",
    "    complex_word_percentage = 0\n",
    "    \n",
    "    for word in tokens:\n",
    "        if nsyl(word.lower()) > 2:\n",
    "                    complex_word_count += 1\n",
    "    if len(tokens) != 0:\n",
    "        complex_word_percentage = complex_word_count/len(tokens)\n",
    "    \n",
    "    return complex_word_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fog Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating Fog Index \n",
    "# Fog index is calculated using -- Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)\n",
    "\n",
    "def fog_index(averageSentenceLength, percentageComplexWord):\n",
    "    fogIndex = 0.4 * (averageSentenceLength + percentageComplexWord)\n",
    "    return fogIndex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting total words\n",
    "\n",
    "def total_word_count(text):\n",
    "    tokens = tokenizer(text)\n",
    "    return len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Personal Pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculating the pronouns presenet in the text. \n",
    " \n",
    "def personal_pronoun(text):\n",
    "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "    pronouns = pronounRegex.findall(text)\n",
    "    # to get all pronouns preseny use only `return pronouns`\n",
    "    return len(pronouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 : Performing Text Analysis and using all the defined functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "      <th>Unnamed: 25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>167</td>\n",
       "      <td>https://insights.blackcoffer.com/role-big-data...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>168</td>\n",
       "      <td>https://insights.blackcoffer.com/sales-forecas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>169</td>\n",
       "      <td>https://insights.blackcoffer.com/detect-data-e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>170</td>\n",
       "      <td>https://insights.blackcoffer.com/data-exfiltra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>171</td>\n",
       "      <td>https://insights.blackcoffer.com/impacts-of-co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  Unnamed: 2  \\\n",
       "0         1  https://insights.blackcoffer.com/how-is-login-...         NaN   \n",
       "1         2  https://insights.blackcoffer.com/how-does-ai-h...         NaN   \n",
       "2         3  https://insights.blackcoffer.com/ai-and-its-im...         NaN   \n",
       "3         4  https://insights.blackcoffer.com/how-do-deep-l...         NaN   \n",
       "4         5  https://insights.blackcoffer.com/how-artificia...         NaN   \n",
       "..      ...                                                ...         ...   \n",
       "165     167  https://insights.blackcoffer.com/role-big-data...         NaN   \n",
       "166     168  https://insights.blackcoffer.com/sales-forecas...         NaN   \n",
       "167     169  https://insights.blackcoffer.com/detect-data-e...         NaN   \n",
       "168     170  https://insights.blackcoffer.com/data-exfiltra...         NaN   \n",
       "169     171  https://insights.blackcoffer.com/impacts-of-co...         NaN   \n",
       "\n",
       "     Unnamed: 3  Unnamed: 4  Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  \\\n",
       "0           NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "1           NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "2           NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "3           NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "4           NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "165         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "166         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "167         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "168         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "169         NaN         NaN         NaN         NaN         NaN         NaN   \n",
       "\n",
       "     Unnamed: 9  ...  Unnamed: 16  Unnamed: 17  Unnamed: 18  Unnamed: 19  \\\n",
       "0           NaN  ...          NaN          NaN          NaN          NaN   \n",
       "1           NaN  ...          NaN          NaN          NaN          NaN   \n",
       "2           NaN  ...          NaN          NaN          NaN          NaN   \n",
       "3           NaN  ...          NaN          NaN          NaN          NaN   \n",
       "4           NaN  ...          NaN          NaN          NaN          NaN   \n",
       "..          ...  ...          ...          ...          ...          ...   \n",
       "165         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "166         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "167         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "168         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "169         NaN  ...          NaN          NaN          NaN          NaN   \n",
       "\n",
       "     Unnamed: 20  Unnamed: 21  Unnamed: 22  Unnamed: 23  Unnamed: 24  \\\n",
       "0            NaN          NaN          NaN          NaN          NaN   \n",
       "1            NaN          NaN          NaN          NaN          NaN   \n",
       "2            NaN          NaN          NaN          NaN          NaN   \n",
       "3            NaN          NaN          NaN          NaN          NaN   \n",
       "4            NaN          NaN          NaN          NaN          NaN   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "165          NaN          NaN          NaN          NaN          NaN   \n",
       "166          NaN          NaN          NaN          NaN          NaN   \n",
       "167          NaN          NaN          NaN          NaN          NaN   \n",
       "168          NaN          NaN          NaN          NaN          NaN   \n",
       "169          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "     Unnamed: 25  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "..           ...  \n",
       "165          NaN  \n",
       "166          NaN  \n",
       "167          NaN  \n",
       "168          NaN  \n",
       "169          NaN  \n",
       "\n",
       "[170 rows x 26 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(inputData)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://insights.blackcoffer.com/how-is-login-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://insights.blackcoffer.com/how-does-ai-h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-and-its-im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://insights.blackcoffer.com/how-do-deep-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://insights.blackcoffer.com/how-artificia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0       1  https://insights.blackcoffer.com/how-is-login-...\n",
       "1       2  https://insights.blackcoffer.com/how-does-ai-h...\n",
       "2       3  https://insights.blackcoffer.com/ai-and-its-im...\n",
       "3       4  https://insights.blackcoffer.com/how-do-deep-l...\n",
       "4       5  https://insights.blackcoffer.com/how-artificia..."
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing out the unused columns \n",
    "df=df[['URL_ID','URL']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the Articles from the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-76-1bf08de9e77e>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataframe['ARTICLE'] = p\n"
     ]
    }
   ],
   "source": [
    "getArticles(df,df.URL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying all the functions(in section 1) above to get the required variables as mentioned in Data Structure provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-105-bb49c7ad97cb>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['positive_score'] = df['ARTICLE'].apply(lambda x : positive_score(x))\n",
      "<ipython-input-105-bb49c7ad97cb>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['negative_score'] = df['ARTICLE'].apply(lambda x : negative_score(x))\n",
      "<ipython-input-105-bb49c7ad97cb>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['polarity_score'] = df['ARTICLE'].apply(lambda x: polarity_score(x))\n",
      "<ipython-input-105-bb49c7ad97cb>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['subjectivity_score'] = df['ARTICLE'].apply(lambda x: subjectivity_score(x))\n",
      "<ipython-input-105-bb49c7ad97cb>:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['average_sentence_length'] = df['ARTICLE'].apply(lambda x : average_sentence_length(x))\n",
      "<ipython-input-105-bb49c7ad97cb>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['percentage_of_complex_words'] = df['ARTICLE'].apply(lambda x : percentage_complex_word(x))\n",
      "<ipython-input-105-bb49c7ad97cb>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['fog_index'] = df['ARTICLE'].apply(lambda x :fog_index(average_sentence_length(x),percentage_complex_word(x)))\n",
      "<ipython-input-105-bb49c7ad97cb>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['avg_word_per_sentence']= df['ARTICLE'].apply(lambda x : avg_num_words_per_sentence(x))\n",
      "<ipython-input-105-bb49c7ad97cb>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['complex_word_count']= df['ARTICLE'].apply(lambda x : complex_word_count(x))\n",
      "<ipython-input-105-bb49c7ad97cb>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['word_count'] = df['ARTICLE'].apply(lambda x : total_word_count(x))\n",
      "<ipython-input-105-bb49c7ad97cb>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['syllable_per_word'] = df['ARTICLE'].apply(lambda x : nsyl(x))\n",
      "<ipython-input-105-bb49c7ad97cb>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['num_of_personal_pronouns'] = df['ARTICLE'].apply(lambda x : personal_pronoun(x))\n",
      "<ipython-input-105-bb49c7ad97cb>:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['average_word_length'] = df['ARTICLE'].apply(lambda x : average_word_length(x))\n"
     ]
    }
   ],
   "source": [
    "df['positive_score'] = df['ARTICLE'].apply(lambda x : positive_score(x))\n",
    "df['negative_score'] = df['ARTICLE'].apply(lambda x : negative_score(x))\n",
    "df['polarity_score'] = df['ARTICLE'].apply(lambda x: polarity_score(x))\n",
    "df['subjectivity_score'] = df['ARTICLE'].apply(lambda x: subjectivity_score(x))\n",
    "df['average_sentence_length'] = df['ARTICLE'].apply(lambda x : average_sentence_length(x))\n",
    "df['percentage_of_complex_words'] = df['ARTICLE'].apply(lambda x : percentage_complex_word(x))\n",
    "df['fog_index'] = df['ARTICLE'].apply(lambda x :fog_index(average_sentence_length(x),percentage_complex_word(x)))\n",
    "df['avg_word_per_sentence']= df['ARTICLE'].apply(lambda x : avg_num_words_per_sentence(x))\n",
    "df['complex_word_count']= df['ARTICLE'].apply(lambda x : complex_word_count(x))\n",
    "df['word_count'] = df['ARTICLE'].apply(lambda x : total_word_count(x))\n",
    "df['syllable_per_word'] = df['ARTICLE'].apply(lambda x : nsyl(x))\n",
    "df['num_of_personal_pronouns'] = df['ARTICLE'].apply(lambda x : personal_pronoun(x))\n",
    "df['average_word_length'] = df['ARTICLE'].apply(lambda x : average_word_length(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drooping the Article Column extracted \n",
    "df.drop('ARTICLE', inplace=True, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the dataframe to an output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('output.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4af1e78d603f7d27cc60a9a5c8ae254cf839c35c66daa31c3a403b164618dcda"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
